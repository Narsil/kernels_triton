//
// Generated by LLVM NVPTX Back-End
//

.version 7.4
.target sm_52
.address_size 64

	// .globl	add_kernel_0d1d2d3d

.visible .entry add_kernel_0d1d2d3d(
	.param .u64 add_kernel_0d1d2d3d_param_0,
	.param .u64 add_kernel_0d1d2d3d_param_1,
	.param .u64 add_kernel_0d1d2d3d_param_2,
	.param .u32 add_kernel_0d1d2d3d_param_3
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<33>;
	.reg .f32 	%f<25>;
	.reg .b64 	%rd<12>;

	ld.param.u64 	%rd7, [add_kernel_0d1d2d3d_param_0];
	ld.param.u64 	%rd8, [add_kernel_0d1d2d3d_param_1];
	mov.u32 	%r25, %tid.x;
	shl.b32 	%r26, %r25, 2;
	ld.param.u64 	%rd9, [add_kernel_0d1d2d3d_param_2];
	and.b32  	%r27, %r26, 1020;
	ld.param.u32 	%r28, [add_kernel_0d1d2d3d_param_3];
	mov.u32 	%r29, %ctaid.x;
	shl.b32 	%r30, %r29, 10;
	or.b32  	%r31, %r27, %r30;
	add.s32 	%r32, %r31, 512;
	mul.wide.s32 	%rd10, %r31, 4;
	add.s64 	%rd1, %rd7, %rd10;
	mul.wide.s32 	%rd11, %r32, 4;
	add.s64 	%rd2, %rd7, %rd11;
	setp.lt.s32 	%p1, %r31, %r28;
	setp.lt.s32 	%p2, %r32, %r28;
	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
	mov.b32 	%f1, %r1;
	mov.b32 	%f2, %r2;
	mov.b32 	%f3, %r3;
	mov.b32 	%f4, %r4;
	@%p2 ld.global.v4.b32 { %r5, %r6, %r7, %r8 }, [ %rd2 + 0 ];
	mov.b32 	%f5, %r5;
	mov.b32 	%f6, %r6;
	mov.b32 	%f7, %r7;
	mov.b32 	%f8, %r8;
	add.s64 	%rd3, %rd8, %rd10;
	add.s64 	%rd4, %rd8, %rd11;
	@%p1 ld.global.v4.b32 { %r9, %r10, %r11, %r12 }, [ %rd3 + 0 ];
	mov.b32 	%f9, %r9;
	mov.b32 	%f10, %r10;
	mov.b32 	%f11, %r11;
	mov.b32 	%f12, %r12;
	@%p2 ld.global.v4.b32 { %r13, %r14, %r15, %r16 }, [ %rd4 + 0 ];
	mov.b32 	%f13, %r13;
	mov.b32 	%f14, %r14;
	mov.b32 	%f15, %r15;
	mov.b32 	%f16, %r16;
	add.s64 	%rd5, %rd9, %rd10;
	add.s64 	%rd6, %rd9, %rd11;
	add.f32 	%f17, %f1, %f9;
	add.f32 	%f18, %f2, %f10;
	add.f32 	%f19, %f3, %f11;
	add.f32 	%f20, %f4, %f12;
	add.f32 	%f21, %f5, %f13;
	add.f32 	%f22, %f6, %f14;
	add.f32 	%f23, %f7, %f15;
	add.f32 	%f24, %f8, %f16;
	mov.b32 	%r17, %f17;
	mov.b32 	%r18, %f18;
	mov.b32 	%r19, %f19;
	mov.b32 	%r20, %f20;
	@%p1 st.global.v4.b32 [ %rd5 + 0 ], { %r17, %r18, %r19, %r20 };
	mov.b32 	%r21, %f21;
	mov.b32 	%r22, %f22;
	mov.b32 	%r23, %f23;
	mov.b32 	%r24, %f24;
	@%p2 st.global.v4.b32 [ %rd6 + 0 ], { %r21, %r22, %r23, %r24 };
	ret;

}
